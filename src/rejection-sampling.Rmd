---
title: "CiC Rejection Sampling"
output: html_notebook
---

This workbook is responsible for creating some useful model inputs:
* Creating a target duration distribution for each join age using survival analysis
* Accounting for the effect of aging out (a dynamic which must be handled separately from survival analysis)
* Creating model input files containing pre-calculated CiC periods which when sampled match the inferred target duration distribution


```{r}
library(lubridate)
library(survival)
library(survminer)
library(stringr)
library(reshape2)
library(zoo)
library(dplyr)
library(tidyr)
```

```{r}
source("helpers.R")
```

```{r}
# First create schema mastodon in database mastodon

library(DBI)
library(RPostgreSQL)
drv <- dbDriver("PostgreSQL")
conn <- dbConnect(drv, dbname = "mastodon")
```

```{r}

extract_date <- as.Date("2020-03-31")
input_csv <- "../data/scc/2021-04-08/suffolk-scrubbed-episodes-20210219.csv"
# All of the periods we have simulated
periods_universe_csv <- "../data/scc/2021-02-24/scc-periods-universe-2019-03-31-segment-interval-2190-rewind-1yr-train-3yr-samples-100-seed-42-age-out-jitter-7.csv"

output_csv <- ".../data/scc/2021-04-08/target-distribution.csv"
age_out_proportions_output_csv <- "../data/scc/2021-04-08/age-out-proportions.csv"

simulated_output_csv <- "../data/scc/2021-04-08/simulated-candidates.csv"
projected_output_csv <- "../data/scc/2021-04-08/projected-candidates.csv"
simulated_age_out_output_csv <- "../data/scc/2021-04-08/simulated-age-out-candidates.csv"
projected_age_out_output_csv <- "../data/scc/2021-04-08/projected-age-out-candidates.csv"

data <- read.csv(input_csv)
data$report_date <- ymd(data$report_date)
data$ceased <- ymd(data$ceased)

data.frame(xs = c(data$report_date, data$ceased)) %>%
  filter(!is.na(xs)) %>%
  filter(xs > as.Date("2020-01-01")) %>%
  ggplot(aes(xs)) + geom_histogram()


# Fix dodgy NAs

# data$ceased[data$ceased > extract_date] <- NA
# data <- data[data$report_date <= extract_date,]
# 
# data %>% filter(period_id == "881-1")
# data %>% filter(period_id == "486-1")
# 
# data %>% filter(period_id == "1815-1")
```

```{r}

# Creates data that will be fed to the survival curve fitting model.
# We only know which month each child's birthday falls in, so we create a record
# for each day of this month.

noise <- 7

survival_data <- data %>%
  mutate(report_date = ymd(report_date),
         ceased = ymd(ceased)) %>%
  group_by(period_id) %>%
  dplyr::summarise(period_start = min(report_date),
                   period_end = max(ceased),
                   birth_month_beginning = ymd(paste0(DOB[1], "-01")),
                   birth_month_end = birth_month_beginning + months(1) - days(1))

# We think the child aged out if they *could* have hit 18 years - 1 day given their birthday range
# That means if we assume the oldest they could be, did that cross 18 years - 1 day.
# That means if we take their earlier birthday, is it *at least* 18 years - 1 day.

survival_data2 <- survival_data %>%
  mutate(birthday_lower = pmax(birth_month_beginning, eighteen_years_before(replace_na(period_end, extract_date))),
         birthday_upper = pmin(birth_month_end, period_start),
         # aged_out = year_diff(birth_month_beginning, replace_na(period_end, extract_date)) >= 18,
         # Removing this period_end assignment - don't think we need it
         # period_end = if_else(aged_out, eighteen_years_after(birthday_upper) - days(1), period_end),
         event = !is.na(period_end),
         duration = if_else(event,
                            day_diff(period_start, period_end),
                            day_diff(period_start, extract_date)))

# These cases violate one of more of our assumptions. We'll assume they left at 18.
survival_data2 %>% filter(birthday_lower > birthday_upper)

survival_data3 <- survival_data2 %>%
  mutate(birthday_lower = pmin(birthday_lower, birthday_upper) # Only matters if there are age 18+ leavers above
         ) %>%
  inner_join(data.frame(n = 1:100), by = character()) %>%
  mutate(birthday = as.Date(impute_birthday(birthday_lower, birthday_upper), origin = "1970-01-01"),
         admission_age = as.factor(clamp(year_diff(birthday, period_start), 0, 17)),
         max_duration = day_diff(period_start, eighteen_years_after(birthday)),
         duration = pmin(duration, max_duration),
         fuzzed_duration = binomial_noise(duration, max_duration, noise)
         ) %>%
  as.data.frame

survival_data4 <- survival_data3 %>%
  mutate(aged_out = year_diff(birthday, period_start + days(fuzzed_duration)) >= 17, # Going to say no one leaves aged 17
         event = event | aged_out, # All aged out children are leavers
         duration = if_else(aged_out, max_duration, duration),
         fuzzed_duration = if_else(aged_out, max_duration, fuzzed_duration),
         fuzzed_exit_age = fuzzed_duration + day_diff(birthday, period_start))

```



```{r}
fit <- survfit(Surv(fuzzed_duration, event) ~ admission_age, data = survival_data4 %>% filter(!aged_out))
quantiles <- reshape2::melt(stats::quantile(fit, probs = seq(0,0.9999,length.out = 10000))$quantile) %>%
  mutate(join_age = as.integer(str_replace(Var1, "admission_age=", ""))) %>%
  select(-1) %>%
  as.data.frame
colnames(quantiles) <- c("quantile", "duration", "admission_age")

quantiles <- dcast(admission_age ~ quantile, data = quantiles, drop = FALSE, value.var = "duration")
for (row in 1:nrow(quantiles)) {
  # I expect there's a better way of doing this
  age <- row - 1
  max_days <- (18 - age) * 365
  quantiles[row,is.na(quantiles[row,])] <- max_days
  quantiles[row,quantiles[row,] > max_days] <- max_days
}
quantiles <- cbind(quantiles, data.frame(`100` = 17:1*365))
colnames(quantiles) <- c("admission_age", seq(0, 10000, length.out = ncol(quantiles) - 1))
quantiles <- melt(quantiles, id.vars = "admission_age") %>%
  mutate(quantile = as.numeric(variable) / 10000.0,
         duration = value) %>%
  select(admission_age, duration, quantile) %>%
  group_by(admission_age, duration) %>%
  summarise(quantile = max(quantile))

target_distribution <- quantiles %>%
  mutate(duration_group = duration %/% 7) %>%
  group_by(admission_age, duration_group) %>%
  summarise(quantile = max(quantile)) %>%
  arrange(admission_age, duration_group) %>%
  mutate(density = quantile - coalesce(lag(quantile), 0)) %>%
  mutate(density = density * 100 / sum(density))



write.csv(target_distribution, file = output_csv, row.names = FALSE)
```

```{r}

# Calculate probability that joiner of each age makes it past 17th birthday

fit <- survfit(Surv(fuzzed_exit_age, event) ~ admission_age, data = survival_data4)
quantiles <- reshape2::melt(stats::quantile(fit, probs = seq(0,0.9999,length.out = 10000))$quantile) %>%
  mutate(join_age = as.integer(str_replace(Var1, "admission_age=", ""))) %>%
  select(-1) %>%
  as.data.frame
colnames(quantiles) <- c("quantile", "exit_age", "admission_age")

age_out_proportions <- quantiles %>%
  group_by(admission_age) %>%
  mutate(age_out_quantile = min(if_else(exit_age > 17 * 365.25, quantile, NA_real_), na.rm = TRUE)) %>%
  mutate(p = age_out_quantile - quantile) %>%
  group_by(admission_age, exit_age) %>%
  summarise(p = (100 - min(p)) / 100.0) %>%
  rename(current_age_days = exit_age)

write.csv(age_out_proportions, file = age_out_proportions_output_csv, row.names = FALSE)

```


```{sql connection=conn}
-- Create the periods table containing the universe of all periods
DROP TABLE IF EXISTS mastodon.periods;

CREATE TABLE mastodon.periods (
    provenance character varying(1),
    id character varying(16),
    sample_index integer,
    admission_age integer,
    admission_age_days integer,
    duration integer,
    episodes_edn text,
    aged_out boolean
);

DROP TABLE IF EXISTS mastodon.target_distribution;

CREATE TABLE mastodon.target_distribution (
    admission_age integer,
    duration_group integer,
    density double precision
);

SELECT TRUE AS created_input_tables;
```

```{r}
# Upload periods universe
periods_universe <- read.csv(periods_universe_csv)

staged <- periods_universe
colnames(staged) <- c("provenance", "id", "sample_index", "admission_age", "admission_age_days", "duration", "episodes_edn", "aged_out")
dbWriteTable(conn, c("mastodon", "periods"), staged, append = TRUE, row.names = FALSE)

# Upload target distribution
staged <- target_distribution[,c("admission_age", "duration_group", "density")]
dbWriteTable(conn, c("mastodon", "target_distribution"), staged, append = TRUE, row.names = FALSE)
```

We create versions of the input periods to try and get good coverage of the whole target distribution.
This means adjusting the total duration by up to 10 weeks either way.
We don't filter periods which go over 18 because we adjust the birthdays in the following section.

```{sql connection=conn}
DROP TABLE IF EXISTS mastodon.fuzzy_periods;

CREATE TABLE mastodon.fuzzy_periods AS
WITH noise AS (
  SELECT generate_series(-10, 10) * 7 AS noise
)
SELECT p.provenance, p.id, p.sample_index, p.admission_age, p.admission_age_days,
  cast(p.duration + noise AS int) AS duration, CAST(duration + noise AS int) / 7 AS duration_group, episodes_edn
FROM mastodon.periods AS p
  CROSS JOIN noise
WHERE (duration + noise >= 28 OR noise = 0)
AND aged_out = FALSE;

INSERT INTO mastodon.fuzzy_periods
WITH noise AS (
  SELECT generate_series(-7, 7) AS noise
)
SELECT p.provenance, p.id, p.sample_index, p.admission_age, p.admission_age_days,
  cast(p.duration + noise AS int) AS duration, CAST(duration + noise AS int) / 7 AS duration_group, episodes_edn
FROM mastodon.periods AS p
  CROSS JOIN noise
WHERE duration + noise >= 0
AND duration + noise < 28
AND noise != 0
AND aged_out = FALSE;

SELECT TRUE AS inserted_fuzzy_periods;
```

For all those who join aged at least 1, we create versions of all simulated and projected data with fuzzed birthdays: up to 5 weeks in either direction.

```{sql connection=conn}
WITH noise AS (
  SELECT generate_series(-5, 5) * 7 AS noise
)
INSERT INTO mastodon.fuzzy_periods
SELECT p.provenance, p.id, p.sample_index, CAST(p.admission_age_days + noise AS int) / 365, CAST(p.admission_age_days + noise AS int) AS admission_age_days, p.duration, p.duration_group, episodes_edn
FROM mastodon.fuzzy_periods AS p
CROSS JOIN noise
WHERE noise != 0 AND CAST(p.admission_age_days + noise AS int) / 365 >= 1
AND provenance IN ('S', 'P');

SELECT TRUE AS inserted_more_fuzzy_periods;
```


```{sql connection=conn}
DROP TABLE IF EXISTS mastodon.candidate_distributions;

DROP TABLE IF EXISTS mastodon.target_projected_distribution;

CREATE TABLE mastodon.candidate_distributions AS
SELECT provenance, admission_age, duration_group, COUNT(*) AS density
FROM mastodon.fuzzy_periods
GROUP BY provenance, admission_age, duration_group;

CREATE TABLE mastodon.target_projected_distribution AS
WITH open_period_durations AS (
  SELECT *, CAST(duration AS int) / 7 AS duration_group
  FROM mastodon.periods
  WHERE provenance = 'C'
),
target_projected_distribution AS (
  SELECT pp.admission_age, td.duration_group, CASE WHEN pp.duration_group = td.duration_group THEN density * (7 - (pp.duration % 7)) ELSE density END AS density
  FROM open_period_durations pp
  INNER JOIN mastodon.target_distribution td
  ON pp.admission_age = td.admission_age
  AND td.duration_group >= pp.duration_group
)
SELECT admission_age, duration_group, SUM(density) AS density
FROM target_projected_distribution
GROUP BY admission_age, duration_group;

SELECT TRUE AS created_candidate_and_target_distributions;

```

We need to ensure that every ID to be projected has at least one period with a non-zero density in the target (otherwise it will never be chosen)

```{r}

target_projected_duration_groups <- dbGetQuery(conn, "SELECT admission_age, duration_group, density
                                  FROM mastodon.target_projected_distribution")

for (age in 0:16) {
  print(ggplot(target_projected_duration_groups %>% filter(admission_age == age), aes(duration_group, density)) +
    geom_bar(stat = "identity") +
    facet_wrap(vars(admission_age), scales = "free_y"))
}

non_join_duration_groups <- dbGetQuery(conn, "WITH empty_ids AS (
SELECT fp.id
FROM mastodon.fuzzy_periods fp
LEFT OUTER JOIN mastodon.target_projected_distribution pd
ON fp.admission_age = pd.admission_age
AND fp.duration_group = pd.duration_group
WHERE fp.provenance = 'P'
GROUP BY fp.id
HAVING SUM(coalesce(pd.density, 0)) = 0
)SELECT id, admission_age, duration_group, COUNT(*) AS density
FROM mastodon.fuzzy_periods
WHERE id IN (SELECT id FROM empty_ids)
AND provenance = 'P'
GROUP BY id, admission_age, duration_group;")

combined_duration_groups <- rbind(cbind(id = "target", target_projected_duration_groups),
      non_join_duration_groups) %>%
  group_by(id, admission_age) %>%
  mutate(density = density / max(density)) 

library(ggthemes)

for (age in 0:17) {
  print(ggplot(combined_duration_groups %>% filter(admission_age == age), aes(duration_group, density, fill = id)) +
  geom_bar(stat = "identity", position = "fill") +
  facet_wrap(vars(admission_age)) +
  scale_fill_manual(values = tableau_color_pal("Tableau 20")(20)))
  
}

## Shows they should all remain until 18

```


```{sql connection=conn}
DROP TABLE IF EXISTS mastodon.simulated_candidates;

DROP TABLE IF EXISTS mastodon.projected_candidates;

-- Update destination if necessary
CREATE TABLE mastodon.projected_candidates AS
WITH source_periods AS (
  -- Update source periods if necessary
  SELECT *
  FROM mastodon.fuzzy_periods
  WHERE provenance = 'P'
),
target_distribution AS (
  -- Update target if necessary
  SELECT * FROM mastodon.target_projected_distribution
),
candidate_distribution AS (
  SELECT admission_age, duration_group, COUNT(*) AS density
  FROM source_periods
  GROUP BY admission_age, duration_group
),
reject_proportions AS (
  SELECT t.admission_age, t.duration_group, t.density / c.density AS reject_ratio
  FROM target_distribution t
  LEFT JOIN candidate_distribution c
  ON c.admission_age = t.admission_age
  AND c.duration_group = t.duration_group
)
SELECT fp.*, rp.reject_ratio
FROM source_periods fp
INNER JOIN reject_proportions rp
ON fp.admission_age = rp.admission_age
AND fp.duration_group = rp.duration_group
WHERE random() <= 0.25;

-- If no samples from a particular candidate made it through rejection sampling,
-- bypass rejection sampling to add them in, because otherwise they will be dropped completely.
INSERT INTO mastodon.projected_candidates
WITH missing_periods AS (
SELECT *, row_number() OVER (PARTITION BY id ORDER BY random()) AS rand_id
FROM mastodon.fuzzy_periods fp
WHERE fp.provenance = 'P'
AND id NOT IN (
  SELECT DISTINCT id
  FROM mastodon.projected_candidates
)
)
-- Maybe we ought to set the duration to cause them to age out at 18 - this seem like their destiny.
-- However this would cause them to remain in their final placement for a long time.
SELECT provenance, id, sample_index, admission_age, admission_age_days, duration, duration_group, episodes_edn, 1.0 AS reject_ratio
FROM missing_periods
WHERE rand_id <= 100;

-- Update destination if necessary
CREATE TABLE mastodon.simulated_candidates AS
WITH source_periods AS (
  -- Update source periods if necessary
  SELECT *
  FROM mastodon.fuzzy_periods
  WHERE provenance = 'S'
),
target_distribution AS (
  -- Update target if necessary
  SELECT * FROM mastodon.target_distribution
),
candidate_distribution AS (
  SELECT admission_age, duration_group, COUNT(*) AS density
  FROM source_periods
  GROUP BY admission_age, duration_group
),
reject_proportions AS (
  SELECT t.admission_age, t.duration_group, t.density / c.density AS reject_ratio
  FROM target_distribution t
  LEFT JOIN candidate_distribution c
  ON c.admission_age = t.admission_age
  AND c.duration_group = t.duration_group
)
SELECT fp.*, rp.reject_ratio
FROM source_periods fp
INNER JOIN reject_proportions rp
ON fp.admission_age = rp.admission_age
AND fp.duration_group = rp.duration_group
WHERE random() <= 0.25;

SELECT TRUE AS created_simulated_and_projected_candidates;

```


```{r}

staged <- dbGetQuery(conn, "SELECT * FROM mastodon.projected_candidates");
write.csv(staged, file = projected_output_csv, row.names = FALSE)

staged <- dbGetQuery(conn, "SELECT * FROM mastodon.simulated_candidates");
write.csv(staged, file = simulated_output_csv, row.names = FALSE)

staged <- dbGetQuery(conn, "SELECT provenance, id, sample_index, admission_age, admission_age_days, duration, episodes_edn
FROM mastodon.periods
WHERE provenance = 'S'
AND aged_out = TRUE
AND admission_age_days + duration > (17 * 365)")
write.csv(staged, file = simulated_age_out_output_csv, row.names = FALSE)


staged <- dbGetQuery(conn, "SELECT provenance, id, sample_index, admission_age, admission_age_days, duration, episodes_edn
FROM mastodon.periods
WHERE provenance = 'P'
AND aged_out = TRUE
AND admission_age_days + duration > (17 * 365)")
write.csv(staged, file = projected_age_out_output_csv, row.names = FALSE)

```
